{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": [], "gpuType": "T4"},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": ["# SA_SLM 추론 노트북\n\n", "## 필요 파일\n", "- `sa_slm_adapter.zip`: 학습 노트북에서 다운로드한 파일\n\n", "## 실행 전\n", "런타임 > 런타임 유형 변경 > **GPU (T4)** 선택"],
      "metadata": {"id": "header"}
    },
    {
      "cell_type": "code",
      "source": ["# 1. 환경 설정\n", "!pip install -q torch transformers accelerate peft bitsandbytes"],
      "metadata": {"id": "install"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 2. Adapter 업로드\n", "from google.colab import files\n", "import zipfile, os\n\n", "print('sa_slm_adapter.zip 파일을 업로드하세요')\n", "uploaded = files.upload()\n\n", "for f in uploaded:\n", "    if f.endswith('.zip'):\n", "        zipfile.ZipFile(f).extractall('.')\n", "        print(f'{f} 압축 해제 완료')\n\n", "print('OK' if os.path.exists('./adapter') else 'adapter 폴더 없음')"],
      "metadata": {"id": "upload"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 3. 모델 로드\n", "import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n", "from peft import PeftModel\n\n", "MODEL = 'Qwen/Qwen2.5-3B-Instruct'\n\n", "bnb = BitsAndBytesConfig(\n", "    load_in_4bit=True,\n", "    bnb_4bit_quant_type='nf4',\n", "    bnb_4bit_compute_dtype=torch.bfloat16\n", ")\n\n", "tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True)\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    MODEL, quantization_config=bnb, device_map='auto', trust_remote_code=True\n", ")\n", "model = PeftModel.from_pretrained(model, './adapter')\n", "model.eval()\n\n", "print('Model loaded!')"],
      "metadata": {"id": "model"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 4. 추론 함수\n", "def ask(prompt):\n", "    msgs = [\n", "        {'role': 'system', 'content': '생기부 설계 전문가. 성공 사례 기반 차별화된 활동 추천.'},\n", "        {'role': 'user', 'content': prompt}\n", "    ]\n", "    txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n", "    inp = tokenizer(txt, return_tensors='pt').to(model.device)\n", "    \n", "    with torch.no_grad():\n", "        out = model.generate(**inp, max_new_tokens=800, temperature=0.7, do_sample=True)\n", "    \n", "    return tokenizer.decode(out[0][inp['input_ids'].shape[1]:], skip_special_tokens=True)\n\n", "print('ask() 함수 준비 완료')"],
      "metadata": {"id": "ask"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 5. 활동 추천 테스트\n", "profile = '''계열: 자연\n", "성적: 3등급대\n", "관심: 인공지능, 데이터사이언스\n", "가치관: AI 윤리, 디지털 격차 해소\n", "목표: 컴퓨터공학'''\n\n", "print(ask('차별화된 활동을 추천하세요.\\n\\n' + profile))"],
      "metadata": {"id": "test1"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 6. 세특 문장 생성\n", "activity = '''과목: 정보\n", "활동: 이미지 분류 데이터셋 클래스 불균형 문제 탐구. 언더/오버샘플링 직접 구현하여 비교 실험.'''\n\n", "print(ask('NEIS 세특 문장으로 작성. 3인칭 서술체.\\n\\n' + activity))"],
      "metadata": {"id": "test2"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 7. 역량 평가\n", "statement = '''문장: 코딩 동아리에서 프로그래밍을 배우고 간단한 프로젝트를 수행함.\n", "전공: 컴퓨터공학'''\n\n", "print(ask('역량 평가 및 보완 제안.\\n\\n' + statement))"],
      "metadata": {"id": "test3"},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": ["# 8. 직접 테스트\n", "my_profile = '''계열: 인문\n", "성적: 2등급대\n", "관심: 경영학, 마케팅\n", "가치관: ESG, 사회적 기업\n", "목표: 경영학'''\n\n", "print(ask('차별화된 활동을 추천하세요.\\n\\n' + my_profile))"],
      "metadata": {"id": "custom"},
      "execution_count": null,
      "outputs": []
    }
  ]
}
