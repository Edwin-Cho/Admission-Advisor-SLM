"""
SA_SLM Gradio UI

Colabì—ì„œ í•™ìŠµí•œ LoRA adapterë¥¼ ë¡œì»¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” UIì…ë‹ˆë‹¤.
sa_slm_adapter.zip ì••ì¶• í•´ì œ í›„ adapter í´ë” ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.

Usage:
    python ui/app.py --adapter ./sa_slm_adapter
"""

import argparse
import getpass
import os
from pathlib import Path
from typing import Iterator, List, Optional, Tuple
from threading import Thread

import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer
from peft import PeftModel


# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
is_loaded = False


def load_model(adapter_path: str) -> str:
    """ëª¨ë¸ ë° LoRA adapter ë¡œë“œ (Colab ì½”ë“œì™€ ë™ì¼)"""
    global model, tokenizer, is_loaded
    
    if not adapter_path.strip():
        return "âŒ adapter ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”"
    
    adapter_path = Path(adapter_path)
    if not adapter_path.exists():
        return f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {adapter_path}"
    
    try:
        BASE_MODEL = "Qwen/Qwen2.5-3B-Instruct"
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
        
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        model = PeftModel.from_pretrained(model, str(adapter_path))
        model.eval()
        
        is_loaded = True
        return f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {adapter_path}"
    
    except Exception as e:
        return f"âŒ ë¡œë“œ ì‹¤íŒ¨: {str(e)}"


def get_runtime_info() -> str:
    lines = ["### Runtime / Device ì •ë³´"]

    try:
        lines.append(f"- **Python**: {__import__('sys').version.split()[0]}")
    except Exception:
        pass

    try:
        lines.append(f"- **torch**: {torch.__version__}")
        lines.append(f"- **mps available**: {torch.backends.mps.is_available()}")
    except Exception as e:
        lines.append(f"- **torch í™•ì¸ ì‹¤íŒ¨**: {e}")

    lines.append(f"- **model loaded**: {is_loaded}")

    if is_loaded and model is not None:
        try:
            param = next(model.parameters())
            lines.append(f"- **model.device**: {getattr(model, 'device', 'N/A')}")
            lines.append(f"- **param.device**: {param.device}")
            lines.append(f"- **param.dtype**: {param.dtype}")
        except Exception as e:
            lines.append(f"- **model íŒŒë¼ë¯¸í„° í™•ì¸ ì‹¤íŒ¨**: {e}")
    else:
        lines.append("- **model/param device**: (ëª¨ë¸ ë¡œë“œ í›„ í™•ì¸ ê°€ëŠ¥)")

    return "\n".join(lines)


def ask(prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str: # max_tokensë¥¼ 500ìœ¼ë¡œ ìˆ˜ì •
    """ì¶”ë¡  í•¨ìˆ˜ (Colab ì½”ë“œì™€ ë™ì¼)"""
    if not is_loaded:
        return "âš ï¸ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”"
    
    msgs = [
        {"role": "system", "content": "ìƒê¸°ë¶€ ì„¤ê³„ ì „ë¬¸ê°€. ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜ ì°¨ë³„í™”ëœ í™œë™ ì¶”ì²œ."},
        {"role": "user", "content": prompt}
    ]
    
    txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    inp = tokenizer(txt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        out = model.generate(
            **inp,
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            top_p=0.9
        )
    
    return tokenizer.decode(out[0][inp["input_ids"].shape[1]:], skip_special_tokens=True)


def ask_stream(
    prompt: str,
    max_tokens: int = 800,  # max_tokensë¥¼ 800ìœ¼ë¡œ ì¦ê°€
    temperature: float = 0.7,
) -> Iterator[Tuple[str, str]]:
    if not is_loaded:
        yield "âš ï¸ ë¨¼ì € ëª¨ë¸ì„ ë¡œë“œí•˜ì„¸ìš”", ""
        return

    yield "ì…ë ¥ êµ¬ì„± ì¤‘...", ""

    msgs = [
        {"role": "system", "content": "ìƒê¸°ë¶€ ì„¤ê³„ ì „ë¬¸ê°€. ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜ ì°¨ë³„í™”ëœ í™œë™ ì¶”ì²œ."},
        {"role": "user", "content": prompt},
    ]

    txt = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
    inp = tokenizer(txt, return_tensors="pt").to(model.device)

    try:
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_special_tokens=True,
            timeout=120,  # timeoutì„ 120ì´ˆë¡œ ì¦ê°€
            skip_prompt=True,
        )
    except TypeError:
        streamer = TextIteratorStreamer(
            tokenizer,
            skip_special_tokens=True,
            timeout=120,  # timeoutì„ 120ì´ˆë¡œ ì¦ê°€
        )

    gen_kwargs = {
        **inp,
        "streamer": streamer,
        "max_new_tokens": max_tokens,
        "temperature": temperature,
        "do_sample": True,
        "top_p": 0.9,
    }

    thread = Thread(target=model.generate, kwargs=gen_kwargs)
    thread.start()

    text = ""
    yield "ìƒì„± ì¤‘...", text
    for token_text in streamer:
        text += token_text
        yield "ìƒì„± ì¤‘...", text

    thread.join()
    yield "ì™„ë£Œ", text


def recommend_activities(track: str, grade_tier: str, interests: str, values: str, target_major: str) -> str:
    """í™œë™ ì¶”ì²œ"""
    profile = f"""ê³„ì—´: {track}
ì„±ì : {grade_tier}
ê´€ì‹¬: {interests}
ê°€ì¹˜ê´€: {values}
ëª©í‘œ: {target_major}"""

    return ask(f"ì°¨ë³„í™”ëœ í™œë™ì„ ì¶”ì²œí•˜ì„¸ìš”.\n\n{profile}")


def recommend_activities_stream(
    track: str,
    grade_tier: str,
    interests: str,
    values: str,
    target_major: str,
) -> Iterator[Tuple[str, str]]:
    profile = f"""ê³„ì—´: {track}
ì„±ì : {grade_tier}
ê´€ì‹¬: {interests}
ê°€ì¹˜ê´€: {values}
ëª©í‘œ: {target_major}"""
    yield from ask_stream(f"ì°¨ë³„í™”ëœ í™œë™ì„ ì¶”ì²œí•˜ì„¸ìš”.\n\n{profile}")


def generate_statement(subject: str, activity: str) -> str:
    """ì„¸íŠ¹ ë¬¸ì¥ ìƒì„±"""
    return ask(f"NEIS ì„¸íŠ¹ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±. 3ì¸ì¹­ ì„œìˆ ì²´.\n\nê³¼ëª©: {subject}\ní™œë™: {activity}", max_tokens=250) # max_tokensë¥¼ 250ìœ¼ë¡œ ìˆ˜ì •


def generate_statement_stream(subject: str, activity: str) -> Iterator[Tuple[str, str]]:
    yield from ask_stream(
        f"NEIS ì„¸íŠ¹ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±. 3ì¸ì¹­ ì„œìˆ ì²´.\n\nê³¼ëª©: {subject}\ní™œë™: {activity}",
        max_tokens=200, # max_tokensë¥¼ 200ìœ¼ë¡œ ìˆ˜ì •
    )


def evaluate_statement(statement: str, target_major: str) -> str:
    """ì—­ëŸ‰ í‰ê°€"""
    return ask(f"ì—­ëŸ‰ í‰ê°€ ë° ë³´ì™„ ì œì•ˆ.\n\në¬¸ì¥: {statement}\nì „ê³µ: {target_major}")


def evaluate_statement_stream(statement: str, target_major: str) -> Iterator[Tuple[str, str]]:
    yield from ask_stream(f"ì—­ëŸ‰ í‰ê°€ ë° ë³´ì™„ ì œì•ˆ.\n\në¬¸ì¥: {statement}\nì „ê³µ: {target_major}")


# ë§ˆì§€ë§‰ ìƒì„± ì…ë ¥ ì €ì¥ (ì¬ìƒì„±ìš©)
last_inputs: dict = {"recommend": {}, "statement": {}, "evaluate": {}}


def copy_to_clipboard(text: str) -> str:
    """ë³µì‚¬ ì™„ë£Œ ë©”ì‹œì§€ ë°˜í™˜ (JSì—ì„œ ì‹¤ì œ ë³µì‚¬ ìˆ˜í–‰)"""
    if not text.strip():
        return "âš ï¸ ë³µì‚¬í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤"
    char_count = len(text)
    return f"âœ… ë³µì‚¬ë¨ ({char_count}ì)"


def create_ui(show_model_tab: bool = True):
    """Gradio UI ìƒì„±"""
    with gr.Blocks(title="SA_SLM") as app:
        gr.Markdown("""
        # ğŸ“ SA_SLM: ìƒê¸°ë¶€ ì„¤ê³„ AI
        
        ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜ìœ¼ë¡œ ì°¨ë³„í™”ëœ í™œë™ì„ ì¶”ì²œí•˜ê³  ì„¸íŠ¹ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        > âš ï¸ ì´ ì‹œìŠ¤í…œì€ **ë³´ì¡° ë„êµ¬**ì…ë‹ˆë‹¤. ìµœì¢… ìƒê¸°ë¶€ í™•ì •ì€ ì»¨ì„¤í„´íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.
        """)
        
        if show_model_tab:
            # ëª¨ë¸ ë¡œë“œ íƒ­
            with gr.Tab("ğŸ”§ ëª¨ë¸ ì„¤ì •"):
                gr.Markdown("### LoRA Adapter ë¡œë“œ")
                gr.Markdown("Colab í•™ìŠµì—ì„œ ë‹¤ìš´ë¡œë“œí•œ `sa_slm_adapter.zip`ì„ ì••ì¶• í•´ì œí•œ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                
                adapter_input = gr.Textbox(
                    label="Adapter ê²½ë¡œ",
                    placeholder="./adapter",
                    value="./adapter"
                )
                load_btn = gr.Button("ëª¨ë¸ ë¡œë“œ", variant="primary")
                load_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
                
                load_btn.click(load_model, inputs=[adapter_input], outputs=[load_status])

                gr.Markdown("### ì‹¤í–‰ í™˜ê²½ í™•ì¸")
                runtime_btn = gr.Button("ì‹¤í–‰ í™˜ê²½ í™•ì¸")
                runtime_output = gr.Markdown()
                runtime_btn.click(get_runtime_info, outputs=[runtime_output])
        
        # í™œë™ ì¶”ì²œ íƒ­
        with gr.Tab("ğŸ¯ í™œë™ ì¶”ì²œ"):
            gr.Markdown("### í•™ìƒ í”„ë¡œí•„ ê¸°ë°˜ í™œë™ ì¶”ì²œ")
            
            with gr.Row():
                with gr.Column():
                    track_input = gr.Dropdown(
                        label="ê³„ì—´",
                        choices=["ìì—°", "ì¸ë¬¸", "ì˜ˆì²´ëŠ¥"],
                        value="ìì—°"
                    )
                    grade_tier_input = gr.Dropdown(
                        label="ì„±ì ",
                        choices=["1ë“±ê¸‰ëŒ€", "2ë“±ê¸‰ëŒ€", "3ë“±ê¸‰ëŒ€", "4ë“±ê¸‰ëŒ€", "5ë“±ê¸‰ëŒ€"],
                        value="2ë“±ê¸‰ëŒ€"
                    )
                    target_major_input = gr.Textbox(
                        label="ëª©í‘œ ì „ê³µ",
                        placeholder="ì»´í“¨í„°ê³µí•™",
                        value="ì»´í“¨í„°ê³µí•™"
                    )
                
                with gr.Column():
                    interests_input = gr.Textbox(
                        label="ê´€ì‹¬ ë¶„ì•¼ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                        placeholder="AI, XAI, AGI, ë¹…ë°ì´í„°, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤",
                        value="AI, XAI, AGI, ë¹…ë°ì´í„°, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤",
                    )
                    values_input = gr.Textbox(
                        label="ê°€ì¹˜ê´€/ì‚¬íšŒ ê´€ì‹¬ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
                        placeholder="AI ìœ¤ë¦¬, ë””ì§€í„¸ ê²©ì°¨ í•´ì†Œ",
                        value="AI ìœ¤ë¦¬, ë””ì§€í„¸ ê²©ì°¨ í•´ì†Œ"
                    )
            
            with gr.Row():
                recommend_btn = gr.Button("í™œë™ ì¶”ì²œ", variant="primary")
                recommend_regen_btn = gr.Button("ğŸ”„ ì¬ìƒì„±", variant="secondary")
                recommend_copy_btn = gr.Button("ğŸ“‹ ë³µì‚¬")
            recommend_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
            recommend_output = gr.Textbox(label="ì¶”ì²œ ê²°ê³¼", lines=10, interactive=False)
            
            def recommend_with_save(*args):
                last_inputs["recommend"] = {
                    "track": args[0], "grade_tier": args[1], "interests": args[2],
                    "values": args[3], "target_major": args[4]
                }
                for status, text in recommend_activities_stream(*args):
                    yield status, text
            
            def recommend_regenerate():
                inp = last_inputs["recommend"]
                if not inp:
                    yield "âš ï¸ ë¨¼ì € ì¶”ì²œì„ ì‹¤í–‰í•˜ì„¸ìš”", ""
                    return
                for status, text in recommend_activities_stream(
                    inp["track"], inp["grade_tier"], inp["interests"], inp["values"], inp["target_major"]
                ):
                    yield status, text
            
            recommend_btn.click(
                recommend_with_save,
                inputs=[track_input, grade_tier_input, interests_input, values_input, target_major_input],
                outputs=[recommend_status, recommend_output]
            )
            recommend_regen_btn.click(
                recommend_regenerate,
                outputs=[recommend_status, recommend_output]
            )
            recommend_copy_btn.click(
                None,
                inputs=[recommend_output],
                js="(text) => { navigator.clipboard.writeText(text); }"
            )
        
        # ì„¸íŠ¹ ë¬¸ì¥ ìƒì„± íƒ­
        with gr.Tab("ğŸ“ ì„¸íŠ¹ ë¬¸ì¥ ìƒì„±"):
            gr.Markdown("### í™œë™ â†’ NEIS ì„¸íŠ¹ ë¬¸ì¥")
            
            subject_input = gr.Textbox(
                label="ê³¼ëª©",
                placeholder="ì •ë³´",
                value="ì •ë³´"
            )
            activity_input = gr.Textbox(
                label="í™œë™ ë‚´ìš©",
                placeholder="ì´ë¯¸ì§€ ë¶„ë¥˜ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ íƒêµ¬. ì–¸ë”/ì˜¤ë²„ìƒ˜í”Œë§ ì§ì ‘ êµ¬í˜„í•˜ì—¬ ë¹„êµ ì‹¤í—˜.",
                lines=3
            )
            
            with gr.Row():
                statement_btn = gr.Button("ë¬¸ì¥ ìƒì„±", variant="primary")
                statement_regen_btn = gr.Button("ğŸ”„ ì¬ìƒì„±", variant="secondary")
                statement_copy_btn = gr.Button("ğŸ“‹ ë³µì‚¬")
            statement_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
            statement_output = gr.Textbox(label="ìƒì„±ëœ ë¬¸ì¥", lines=5, interactive=False)
            
            def statement_with_save(subject, activity):
                last_inputs["statement"] = {"subject": subject, "activity": activity}
                for status, text in generate_statement_stream(subject, activity):
                    yield status, text
            
            def statement_regenerate():
                inp = last_inputs["statement"]
                if not inp:
                    yield "âš ï¸ ë¨¼ì € ë¬¸ì¥ ìƒì„±ì„ ì‹¤í–‰í•˜ì„¸ìš”", ""
                    return
                for status, text in generate_statement_stream(inp["subject"], inp["activity"]):
                    yield status, text
            
            statement_btn.click(
                statement_with_save,
                inputs=[subject_input, activity_input],
                outputs=[statement_status, statement_output]
            )
            statement_regen_btn.click(
                statement_regenerate,
                outputs=[statement_status, statement_output]
            )
            statement_copy_btn.click(
                None,
                inputs=[statement_output],
                js="(text) => { navigator.clipboard.writeText(text); }"
            )
        
        # ì—­ëŸ‰ í‰ê°€ íƒ­
        with gr.Tab("ğŸ“Š ì—­ëŸ‰ í‰ê°€"):
            gr.Markdown("### ì„¸íŠ¹ ë¬¸ì¥ ì—­ëŸ‰ í‰ê°€")
            
            eval_statement_input = gr.Textbox(
                label="í‰ê°€í•  ë¬¸ì¥",
                placeholder="ì½”ë”© ë™ì•„ë¦¬ì—ì„œ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•¨.",
                lines=3
            )
            eval_major_input = gr.Textbox(
                label="ëª©í‘œ ì „ê³µ",
                placeholder="ì»´í“¨í„°ê³µí•™",
                value="ì»´í“¨í„°ê³µí•™"
            )
            
            with gr.Row():
                evaluate_btn = gr.Button("í‰ê°€í•˜ê¸°", variant="primary")
                evaluate_regen_btn = gr.Button("ğŸ”„ ì¬ìƒì„±", variant="secondary")
                evaluate_copy_btn = gr.Button("ğŸ“‹ ë³µì‚¬")
            evaluate_status = gr.Textbox(label="ìƒíƒœ", interactive=False)
            evaluate_output = gr.Textbox(label="í‰ê°€ ê²°ê³¼", lines=10, interactive=False)
            
            def evaluate_with_save(statement, major):
                last_inputs["evaluate"] = {"statement": statement, "major": major}
                for status, text in evaluate_statement_stream(statement, major):
                    yield status, text
            
            def evaluate_regenerate():
                inp = last_inputs["evaluate"]
                if not inp:
                    yield "âš ï¸ ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”", ""
                    return
                for status, text in evaluate_statement_stream(inp["statement"], inp["major"]):
                    yield status, text
            
            evaluate_btn.click(
                evaluate_with_save,
                inputs=[eval_statement_input, eval_major_input],
                outputs=[evaluate_status, evaluate_output]
            )
            evaluate_regen_btn.click(
                evaluate_regenerate,
                outputs=[evaluate_status, evaluate_output]
            )
            evaluate_copy_btn.click(
                None,
                inputs=[evaluate_output],
                js="(text) => { navigator.clipboard.writeText(text); }"
            )
        
        gr.Markdown("""
        ---
        **SA_SLM** | Colab í•™ìŠµ ê²°ê³¼ë¥¼ ë¡œì»¬ì—ì„œ ì‚¬ìš©
        """)
    
    return app


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--adapter", type=str, default="", help="LoRA adapter ê²½ë¡œ")
    parser.add_argument("--port", type=int, default=7860, help="ì„œë²„ í¬íŠ¸")
    parser.add_argument("--share", action="store_true", help="ê³µìœ  ë§í¬ ìƒì„±")
    parser.add_argument("--auth", action="store_true", help="ì ‘ì† ì¸ì¦ ì‚¬ìš©")
    parser.add_argument("--auth-user", type=str, default="", help="ì ‘ì† ì‚¬ìš©ìëª…")
    parser.add_argument("--auth-pass", type=str, default="", help="ì ‘ì† ë¹„ë°€ë²ˆí˜¸")
    args = parser.parse_args()
    
    # ì‹œì‘ ì‹œ adapter ìë™ ë¡œë“œ
    if args.adapter:
        print(load_model(args.adapter))

    app = create_ui(show_model_tab=not args.share)

    auth: Optional[list[Tuple[str, str]]] = None
    if args.auth:
        user = args.auth_user.strip() or os.environ.get("SA_SLM_UI_USER", "").strip() or "admin"
        password = args.auth_pass.strip() or os.environ.get("SA_SLM_UI_PASS", "").strip()
        if not password:
            try:
                password = getpass.getpass("SA_SLM UI password: ")
            except Exception:
                password = ""
        if not password:
            raise SystemExit("--auth ì‚¬ìš© ì‹œ ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. --auth-pass ë˜ëŠ” SA_SLM_UI_PASSë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        print(f"Auth enabled. Username: {user}")
        auth = [(user, password)]

    app.queue().launch(server_port=args.port, share=args.share, auth=auth)
